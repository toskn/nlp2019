{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging\n",
    "На этом семинаре мы рассмотрим некоторые библиотеки, позволяющие делать морфологический анализ, и даже обучим свой теггер на основе [BERT](https://habr.com/ru/post/436878/) (stay tuned!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "Разнообразные теггеры находятся в модуле `nltk.tag`. Можно скачивать дополнительные модели (вспомним, как это делается?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tag\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# корпуса делятся на части - categories\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея 1 : давайте приписывать всем словам один самый частый тег. Его легко найти с помощью `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = nltk.tag.DefaultTagger('NN')\n",
    "sentence = brown.sents()[5]\n",
    "print(sentence)\n",
    "default_tagger.tag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень здорово, верно? Давайте хранить самые частые слова и приписывать им их самый частый тег (а остальным пока приписывать `NN`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = fd.most_common(1000)\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
    "unigram_tagger = nltk.UnigramTagger(model=likely_tags, backoff=default_tagger)\n",
    "unigram_tagger.tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно легко оценить на каких-нибудь предложениях из того же корпуса\n",
    "unigram_tagger.evaluate(brown.tagged_sents(categories='reviews'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно просто приписывать самый частый тег всем словам из корпуса\n",
    "unigram_tagger = nltk.UnigramTagger(brown.tagged_sents(categories='news'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "Используя [документацию](http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.BigramTagger) и функцию `help()` обучите биграммный теггер. Сравните его с униграммным на примерах, где должно стать лучше (выбор тега зависит от контекста), например: `They wind back the clock.` vs. `The wind rises.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо этого в NLTK реализовано множество теггеров:\n",
    "* regexp tagger\n",
    "* ngram tagger\n",
    "* Brill tagger\n",
    "* CRF tagger\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "[SpaCy](https://spacy.io/) — библиотека для обработки текстов, содержащая разные модули (токенайзер, pos-tagging, NER etc.)\n",
    "Разработана специально для построения пайплайнов, сейчас нас будет интересовать токенизация и морфологический анализ.\n",
    "Недостаток — некоторые модели (например, для NER) нужно загружать отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text_id = webtext.fileids()[0]\n",
    "sentences = webtext.raw(first_text_id).split('\\r\\n')\n",
    "text = \" \".join(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Загружаем весь пайплайн для английского\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Обрабатываем текст\n",
    "doc = nlp(text)\n",
    "\n",
    "# Выведем токены, леммы и теги\n",
    "for i, s in enumerate(doc.sents):\n",
    "    print(\"\\n-- Sentence %d --\" % i)\n",
    "    for t in s:\n",
    "        print(t.text, t.lemma_, t.pos_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самостоятельно:\n",
    "возьмите любой текст на английском языке (например, скопируйте абзац из Википедии), примените к нему пайплайн spaCy, \n",
    "выведите все биграммы, состоящие из прилагательного и существительного.\n",
    "\n",
    "**Бонус-трек**: перепишите ваш код в виде функции, которая принимает на вход текст и последовательность тегов, а возвращает энграммы, соответствующие данной последовательности. Протестируйте функцию на 3/4-граммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"\"\"\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair\n",
    "[Flair](https://github.com/zalandoresearch/flair) — библиотека для работы с векторными представлениями слов, содержит компоненты для решения \n",
    "разных задач NLP (sequence tagging, classification ...). Есть как готовые модели, так и возможность обучения на своих данных, дообучение предобученные векторных представлений и т.д.\n",
    "\n",
    "Сегодня рассмотрим некоторые возможности flair для морфологического анализа на примере англоязычных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем простейшие объекты\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# создаем предложение\n",
    "sentence = Sentence('Moscow is the capital of Russia .')\n",
    "\n",
    "# загружаем модель теггинга\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "# обрабатываем предложение\n",
    "# ВАЖНО: объект `sentence` при этом меняется\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(sentence)\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если интересно узнать подробнее об этих объектах\n",
    "help(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У flair есть встроенные датасеты (`flair.datasets`), можно попробовать использовать их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_ENGLISH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.test[0].to_tagged_string('upos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus.test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "n = choice(range(len(corpus.test)))\n",
    "sentence = corpus.test[n]\n",
    "print(sentence.to_tagged_string('upos'))\n",
    "print()\n",
    "\n",
    "# помним, что теперь `sentence` поменяется\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(sentence)\n",
    "print(sentence.to_tagged_string('upos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец: **flair** позволяет обучать свои модели. Попробуем использовать эмбеддинги от Google, чтобы обучить свой теггер, вот [так](https://colab.research.google.com/drive/1OZN14wo1QGiwSpFhYuKPOPbzmJ2CzeWU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
